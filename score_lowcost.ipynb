{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sie21/ExamenBigData/blob/master/score_lowcost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfxweRwhl8Tb",
        "outputId": "89de6dd0-ccab-4e48-9030-30bf2358d414"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-76de0503098e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_spark_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapp_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'4g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    os.remove(\"metastore_db/db.lck\")\n",
        "    os.remove(\"metastore_db/dbex.lck\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def build_spark_session(app_name, memory='4g', executors=4):\n",
        "    return SparkSession.builder\\\n",
        "                      .appName(app_name)\\\n",
        "                      .config('spark.executor.memory', memory)\\\n",
        "                      .config('spark.executor.instances', executors)\\\n",
        "                      .getOrCreate()\n",
        "\n",
        "spark_session = build_spark_session(app_name='ok-google')\n",
        "\n",
        "from pyspark.sql import functions as f\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "U6WHwYu6l8Th"
      },
      "source": [
        "perimetre: représente les identifaints des clients accessible à l'étude.\n",
        "histo_client: represente l'historique des données clients sur une période donnée\n",
        "histo_train: represente l'historique des données de commandes trains.\n",
        "histo_lowcost: represente l'historique des données de client lowcost (défini avec le métier).\n",
        "visites: représente l'historique des données de navigation des clients sur le site."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "bjHk1YMil8Ti"
      },
      "source": [
        "1 - lire les fichiers de données\n",
        "2 - identifier les variables continues et transformer leurs modalités en double.\n",
        "3 - joindre les differentes sources de données en se basant sur les données du périmètre (tous les individus du périmèetre devront apparaitre dans la jointure avec des valeurs NULL si nécessaire pour les colonnes en provenance d'autres sources).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hSy0Mill8Tj"
      },
      "source": [
        "1 - joindre les dataframe sur la clé ID_CLIENT en concervant tous les clients du périmètre.\n",
        "2 - compter le nombre de ID_CLIENT et vérifier qu'il correspond aux nombre d'ID_CLIENT dans la variable perimètre.\n",
        "3 - Caster les variables continues en double et sauvergarder alors le df obtenu dans le repertoire data sur le cluster.\n",
        "4 - Pour les variables catégorielles, créer une nouvelle variable qui prend la modalité de la variable courante si elle existe et \"NA\" sinon.\n",
        "5- Verifier la cohérence des variables continue. Par exemple pour une variable comme age mettre à -1 tous les ages <0 ou>120ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5zD3Terol8Tk"
      },
      "outputs": [],
      "source": [
        "perimetre = spark_session.read.csv(\"data_dauphine/sample_perimetre.csv\", header=True)\n",
        "histo_client_raw = spark_session.read.csv(\"data_dauphine/sample_histo_client.csv\", header=True)\n",
        "histo_train_raw = spark_session.read.csv(\"data_dauphine/sample_histo_train.csv\", header=True)\n",
        "histo_lowcost_raw = spark_session.read.csv(\"data_dauphine/sample_histo_lowcost.csv\", header=True)\n",
        "visites_raw = spark_session.read.csv(\"data_dauphine/sample_visites.csv\", header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcEIAApml8Tk"
      },
      "source": [
        "ecrire une fonction qui cast les variables [\"anciennete\", \"recence_cmd\", \"AGE\"] en float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otyx_Ouil8Tl"
      },
      "source": [
        "le type des variables ci dessous restants inchangé\n",
        "client_cols_to_keep = [\"ID_CLIENT\", 'LBL_STATUT_CLT','LBL_GEO_AIR',\n",
        "            'LBL_SEG_COMPORTEMENTAL','LBL_GEO_TRAIN','LBL_GRP_SEGMENT_NL',\n",
        "            'LBL_SEGMENT_ANTICIPATION','FLG_CMD_CARTE_1225']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "VS5uC0krl8Tm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "histo_train = cast_columns_of_df(histo_train_raw, histo_train_raw.columns,\n",
        "                                 [\"ID_CLIENT\"], cast_type='double')\n",
        "histo_lowcost = cast_columns_of_df(histo_lowcost_raw, histo_lowcost_raw.columns,\n",
        "                                 [\"ID_CLIENT\"], cast_type='double')\n",
        "\n",
        "visites = cast_columns_of_df(visites_raw, visites_raw.columns,\n",
        "                             [\"ID_CLIENT\"], cast_type='double')\n",
        "\n",
        "histo_client = cast_columns_of_df(histo_client_raw,\n",
        "                                  [\"anciennete\", \"recence_cmd\", \"AGE\"],\n",
        "                                  client_cols_to_keep,\n",
        "                                 cast_type='double')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQj55fkWl8Tn"
      },
      "source": [
        "faire une jointure sur le champs client_id entre tous les dataframe en conservant tous les individus du fichier perimetre\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "_x948YE-l8Tn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EecxVSaAl8To"
      },
      "source": [
        "continuous_columns = ['anciennete',\n",
        " 'recence_cmd',\n",
        " 'AGE',\n",
        " 'nb_od',\n",
        " 'mean_nb_passagers',\n",
        " 'mean_duree_voyage',\n",
        " 'mean_mt_voyage',\n",
        " 'mean_tarif_loisir',\n",
        " 'mean_classe_1',\n",
        " 'mean_pointe',\n",
        " 'mean_depart_we',\n",
        " 'flg_track_nl',\n",
        " 'days_since_last_visit',\n",
        " 'tx_conversion']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vy7W7LNl8To"
      },
      "source": [
        "remplacer les valeurs manquantes par '-1' sur toutes les variables categorielles et par -1 sur les variables continues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7lvDHW2l8Tp"
      },
      "source": [
        "Pour la variables LBL_GEO_TRAIN, creer une variable geo_train, qui garde les meme modalités que LBL_GEO_TRAIN\n",
        "si les modalites sont dans la liste ['Toulouse', 'Lille', 'Dijon',\n",
        "                                  'Lyon', 'Marseille', 'Paris',\n",
        "                                  'Nice', 'Limoges','Rouen','Rennes',\n",
        "                                  'Montpellier', 'Bordeaux', 'Metz',\n",
        "                                  'Strasbourg']\n",
        "et prend la valeur 'na' pour les modalites ['Aéroports de Paris Orly',\n",
        "                                'Aéroport de Bâle-Mulhouse / Bassel',\n",
        "                                'Aéroport Lille Lesquin', 'Aéroport de Rennes',\n",
        "                                'Aéroport de Nantes Atlantique',\n",
        "                                'Aéroport de Marseille Provence  (MRS)', \n",
        "                                'Aéroport de Bordeaux Mérignac',\n",
        "                                'Aéroports de Paris Roissy-Charles-de Gaulle', \n",
        "                                \"Aéroport de Nice Côte d'Azur\",\n",
        "                                'Aéroport de Strasbourg',\n",
        "                                'Aéroport de Lyon - Saint Exupéry', \n",
        "                                'Aéroport de Toulouse Blagnac']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VecYoqKsl8Tp"
      },
      "source": [
        "Quels sont les modalites des features LBL_STATUT_CLT et LBL_SEG_COMPORTEMENTAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwoCy2GMl8Tp"
      },
      "source": [
        "Regrouper les modalites de LBL_STATUT_CLT en 4 groupes grands clients, petits clients, prospect et moyen client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwQKKDfjl8Tp",
        "outputId": "06ceab07-4021-46b8-dd84-1c759e53f26a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(flg_cmd_lowcost='1.0'),\n",
              " Row(flg_cmd_lowcost='-1'),\n",
              " Row(flg_cmd_lowcost=None)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1.select(\"flg_cmd_lowcost\").distinct().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kD73M8nql8Tq"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnFsxS2bl8Tq"
      },
      "source": [
        "##### features engineering et modélisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sFuUrBykl8Tq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocessed_df(df, label=\"flg_cmd_lowcostIndex\"):\n",
        "    max_values_to_define_str_cols = 10\n",
        "    id_col = 'ID_CLIENT'\n",
        "    \n",
        "    dty = dict(df.dtypes)\n",
        "    str_cols = [k for k, v in dty.items() if v == 'string']\n",
        "    str_cols.remove(id_col)\n",
        "    \n",
        "    for c in str_cols:\n",
        "        stringIndexer = StringIndexer(inputCol=c, outputCol=c+\"Index\")\n",
        "        model_str = stringIndexer.fit(df)\n",
        "        df = model_str.transform(df).drop(c)\n",
        "\n",
        "    input_cols = df.columns\n",
        "    input_cols.remove(id_col)\n",
        "    input_cols.remove(label)\n",
        "    \n",
        "    assembler = VectorAssembler(inputCols=input_cols,\n",
        "                            outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    \n",
        "    featureIndexer = VectorIndexer(inputCol=\"features\", \n",
        "                   outputCol=\"indexedFeatures\", \n",
        "                   maxCategories=max_values_to_define_str_cols).fit(df)\n",
        "    return featureIndexer.transform(df), df\n",
        "\n",
        "\n",
        "data, dff = preprocessed_df(df)\n",
        "\n",
        "data1 = data.sample(False, 0.1, 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZUWs_mol8Tr"
      },
      "source": [
        "#### Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c-2YkoYHl8Tr"
      },
      "outputs": [],
      "source": [
        "lr = LogisticRegression(labelCol=\"flg_cmd_lowcostIndex\", \n",
        "                        featuresCol=\"indexedFeatures\",elasticNetParam=0.5)\n",
        "lr_model = lr.fit(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHT9IARkl8Tr"
      },
      "outputs": [],
      "source": [
        "lr_model.coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgvnXrTMl8Tr"
      },
      "outputs": [],
      "source": [
        "reg = lr_model.evaluate(data1)\n",
        "reg.areaUnderROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tJK-Z6a2l8Ts"
      },
      "outputs": [],
      "source": [
        "reg.recallByThreshold.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFQfZdWMl8Ts"
      },
      "source": [
        "##### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AK2Mq3Zbl8Ts"
      },
      "outputs": [],
      "source": [
        "classifier = RandomForestClassifier(labelCol=\"flg_cmd_lowcostIndex\", \n",
        "                                    featuresCol=\"indexedFeatures\",\n",
        "                                    maxDepth=15, numTrees=100)\n",
        "\n",
        "model_rf = classifier.fit(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rqG39xGbl8Ts"
      },
      "outputs": [],
      "source": [
        "df_prediction = model_rf.transform(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NiFbni08l8Ts"
      },
      "outputs": [],
      "source": [
        "df_prediction = df_prediction.withColumnRenamed(\"flg_cmd_lowcostIndex\", \"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "qLyaVUL2l8Tt"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSChgLd5l8Tt"
      },
      "outputs": [],
      "source": [
        "evaluator.evaluate(df_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6k8V0Hnl8Tt"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "scoreAndLabels = map(lambda x: (Vectors.dense([1.0 - x[0], x[0]]), x[1]),\n",
        "                     [(0.1, 0.0), (0.1, 1.0), (0.4, 0.0), (0.6, 0.0), (0.6, 1.0), (0.6, 1.0), (0.8, 1.0)])\n",
        "dataset = spark_session.createDataFrame(scoreAndLabels, [\"raw\", \"label\"])\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EB25wDxll8Tt"
      },
      "outputs": [],
      "source": [
        "df_pred = df_prediction.select(\"label\", \"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VoHg01mpl8Tu"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_pred.groupby(\"label\").agg(f.count(\"*\")).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bvAGhpIl8Tu"
      },
      "outputs": [],
      "source": [
        "n = df_pred.count()\n",
        "n1 = df_pred.filter(df_pred.label!=df_pred.prediction).count()\n",
        "n2 = df_pred.filter(df_pred.label==df_pred.prediction).count()\n",
        "n2/float(n) , n1/float(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV7VnsgTl8Tv",
        "outputId": "a83c4546-e10b-4e67-a200-b29ed9e67061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[label: double, prediction: double]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_pred.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwTKWdl8l8Tv",
        "outputId": "1090d182-2b52-4be2-be3d-9e16c77e77a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(98128, 10331)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nb_positif = df_pred.filter(\"label=1\").count()\n",
        "nb_negatif = df_pred.filter(\"label=0\").count()\n",
        "nb_negatif, nb_positif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5_95AaNNl8Tw"
      },
      "outputs": [],
      "source": [
        "pred_positif = df_pred.filter(\"prediction=1\").count()\n",
        "pred_negatif = df_pred.filter(\"prediction=0\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bguCsin2l8Tw"
      },
      "outputs": [],
      "source": [
        "vrai_positif = df_pred.filter(df_pred.label==df_pred.prediction)\\\n",
        "                      .filter(\"prediction=1\").count()\n",
        "faux_positif = df_pred.filter(df_pred.label!=df_pred.prediction)\\\n",
        "                  .filter(\"prediction=1\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6wt92ZUOl8Tw"
      },
      "outputs": [],
      "source": [
        "vrai_negatif = df_pred.filter(df_pred.label==df_pred.prediction)\\\n",
        "                      .filter(\"prediction=0\").count()\n",
        "faux_negatif = df_pred.filter(df_pred.label!=df_pred.prediction)\\\n",
        "                  .filter(\"prediction=0\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5f4oAMyl8Tx",
        "outputId": "e6e0a08a-1a35-40cc-8ff6-98522fb9b45b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 0.6846384667505566, 1.0, 0.9699609990872127)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "recall = vrai_positif/(vrai_positif + faux_positif)\n",
        "precision = sensibility = vrai_positif/ (vrai_positif + faux_negatif)\n",
        "specificity = vrai_negatif/(vrai_negatif + faux_positif)\n",
        "score = (vrai_positif + vrai_negatif)/n\n",
        "recall, precision, specificity, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "et1bgzKll8Tx"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7fEV-r06l8Ty"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il4ttRncl8Ty",
        "outputId": "6701a961-01fb-4140-b923-de479a2f01c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(ID_CLIENT='000843db32fbaecfbb047ca0bb04b1f9f4d9425a', anciennete='1550', recence_cmd='36', AGE=None, LBL_STATUT_CLT='Grand', LBL_GEO_AIR='Aéroports de Paris Orly', LBL_GRP_SEGMENT_NL='Spectateur', LBL_SEG_COMPORTEMENTAL='Chasseurs Bons Plans', LBL_GEO_TRAIN='Paris', LBL_SEGMENT_ANTICIPATION='Mixte', FLG_CMD_CARTE_1225='0')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = spark_session.read.csv(\"data_dauphine/sample_histo_client.csv\", header=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgm2nMYil8Ty"
      },
      "source": [
        "#### how to create a spark data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mxuxmHgl8Ty",
        "outputId": "08c707f6-216b-4251-8355-941f3cf10c16"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style>\n",
              "    .dataframe thead tr:only-child th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>date</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>Ankit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22</td>\n",
              "      <td>2018-02-03</td>\n",
              "      <td>Jalfaizy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>2018-01-05</td>\n",
              "      <td>saurabh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26</td>\n",
              "      <td>2018-01-12</td>\n",
              "      <td>Bala</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age        date      name\n",
              "0   25  2018-01-03     Ankit\n",
              "1   22  2018-02-03  Jalfaizy\n",
              "2   20  2018-01-05   saurabh\n",
              "3   26  2018-01-12      Bala"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "import datetime\n",
        "\n",
        "l = [(datetime.date(2018,1,3), 'Ankit',25),\n",
        "     (datetime.date(2018,2,3), 'Jalfaizy',22),\n",
        "     (datetime.date(2018,1,5), 'saurabh',20),\n",
        "     (datetime.date(2018,1,12), 'Bala',26)]\n",
        "rdd = spark_session.sparkContext.parallelize(l)\n",
        "people = rdd.map(lambda x: Row(date=x[0], name=x[1], age=int(x[2])))\n",
        "schemaPeople = spark_session.createDataFrame(people)\n",
        "schemaPeople.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaSf9yPPl8Ty",
        "outputId": "62c2f8cb-7837-48eb-8e5f-e653219d4b62"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style>\n",
              "    .dataframe thead tr:only-child th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>date</th>\n",
              "      <th>name</th>\n",
              "      <th>date_max</th>\n",
              "      <th>days_since_last_visit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>Ankit</td>\n",
              "      <td>2018-02-21</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22</td>\n",
              "      <td>2018-02-03</td>\n",
              "      <td>Jalfaizy</td>\n",
              "      <td>2018-02-21</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>2018-01-05</td>\n",
              "      <td>saurabh</td>\n",
              "      <td>2018-02-21</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>26</td>\n",
              "      <td>2018-01-12</td>\n",
              "      <td>Bala</td>\n",
              "      <td>2018-02-21</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age        date      name    date_max  days_since_last_visit\n",
              "0   25  2018-01-03     Ankit  2018-02-21                     49\n",
              "1   22  2018-02-03  Jalfaizy  2018-02-21                     18\n",
              "2   20  2018-01-05   saurabh  2018-02-21                     47\n",
              "3   26  2018-01-12      Bala  2018-02-21                     40"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dd = schemaPeople.select(\"*\",\n",
        "                         f.lit(datetime.date.today()).alias(\"date_max\"))\n",
        "dd.select(\"*\", f.datediff('date_max', 'date')\\\n",
        "                    .alias('days_since_last_visit')).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UKcXmjI1l8Ty"
      },
      "outputs": [],
      "source": [
        "# Given a dice of 6 sides we play the following game:\n",
        "#   Roll the dice until you observe all 6 sides.\n",
        "#   The score of the game is the sum of your rolls.\n",
        "# Estimate the average game score\n",
        "# Example: 2 3 6 1 4 2 3 6 4 1 2 5 --> score: 2+3+6+1+...+1+2+5\n",
        "# S is a R.V. --> estimate E(S)=?\n",
        "\n",
        "\n",
        "# Step 0: simulate a single roll of a dice\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "s = []\n",
        "for j in range(100):\n",
        "    score = []\n",
        "    for i in range(100):\n",
        "        one_roll = random.randint(1,6)\n",
        "        score.append(one_roll)\n",
        "    s.append(np.sum(np.array(score)))\n",
        "np.mean(np.array(np.array(s)))\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "score_lowcost.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}